#+LATEX_CLASS: article

#+TITLE: Problem Solving with Algorithms
#+AUTHOR: Michael Mattie

* Problem Solving with Algorithms

#+BEGIN_CENTER
*Correct Thinking leads to correct Code!*
#+END_CENTER

* Correctness

Not all algorithms require a major effort to get a correct
result. However these algorithms that are trivial are almost entirely
of a purely technical aspect with a narrow effect on the program.

The algorithms that deal with Business Problems on the other hand
are unique to the Problem Domain and can be very difficult to
get right, and mistakes can ripple throughout the implementation.

In these cases where the blast radius is high a rigorous approach can
save a project weeks or more of endless tweaking and ad-hoc testing
combined into a disaster.

Read further for a template that can design techincal algorithms
or business logic, in other words design at any scale.

* Design Process

** Definition

- SCENARIOS – The context that defines the CASES and BEHAVIOR
- BEHAVIOR – State & Clarify
- INPUTS – State & Clarify
- RETURN – answers in computational results - types and scale
- CHANGE – effects in state or behavior

** Design (Iteration)

- Come up with CASES to test ideas
- Conduct thought experiments, Give up on bad ideas quickly 
- maximize idempotent side-effect free functions (API) [[cite:&SICPcostOfAssignment]]
- Objects represent state (modality)
- Functions are isolated, minimal sharing of state between functions (API)

** Paradigm (Comprehension)

Paradigm is what model is best suited to framing the problem (dynamic
greedy, lazy, streams, Relational, divide and conquer.

Spot check the model to see if it adequately describes the
problem. Find the model that underlies the Solution.

*** Recursion

\begin{equation}
\theta(\log_n)
\end{equation}

- recurrence: distill the problem down into a solution that can be applied to
  all the elements.
- termination: define the base case or *termination* as return of the solution
  that unwinds the recursion.
- traverse: define the traverse of the problem set.

Recursion is a powerful technique however it is rarely used in stack
based lanugages due to the fact that deep traverses can blow a fixed
length stack.

*** Divide & Conquer

Divide and Conquer is a technique where the problem is dived into parts,
each part is solved, and then the sub-solutions are combined into
the complete solution.

\begin{equation}
\theta (n * \log_n)
\end{equation} 

- Divide the problem into $n/x$ parts.
- Solve each part
- Combine the solutions for the final solution

*** dynamic

Dynamic Programming uses a technique of caching answers to frequently
computed problems.

Memoization[[cite:&IntroMemoization]] is a powerful technique and
in Python the "functools" package has a LRU [[cite:&IntroLRU]]

*** linguistic (DSL)

DSL Stands for Domain Specific Languages. Thes can be simple
declarative language processors, or full blown domain specific
languages like "R" [[cite:&WikiR]].

*** query

Query Languages like SQL can go beyond transactional into the space of
analytical queries either providing processing of data, or even
computations such as "GROUP BY" and MIN and MAX in SQL [[cite:&WikiSQL]].

The underlying model behind relational databases is the Relational
Algebra [[cite:&codd2021relational]]

*** logic

Logic systems are basically rule systems like Prolog [[cite:&WikiProlog]]
They are used in mathematical and logic applications. Their solution
finding approach can also be useful in solving difficult problems like
cross-wiring network links for redundancy and expert systems.

*** single pass

Single pass approaches are significant when the data set is so large it
cannot be contained in memory. These kinds of problems are becoming
more important as the size of data in general skyrockets.

*** multi-pass

Sometimes huge gains can be made by making multiple passes. This is
basically a variant on Dynamic Programming. A good example is database
indexes. When data is queried the location can be found quickly in
the index instead of a full table scan.

Sorting ahead of time is another example, making possible a Binary
Search technique.

*** pre-compute

Pre-Computing is not like multi-pass in that the complete problem
set is traversed, rather it is the compilation of tables that
are expensive to compute. In the early days of computing the
computation of sine/cosine and other graphic operations
were prohibitely expensive. 

Since the answers were a small table pre-computing the results of the
equations greatly sped up programs. Bitmaps were even compiled to
machine code for faster rendering.

*** multi-process

There is an entire field of programming dedicated to muli-process
computing. It is based upon parallel computation which is currently in
vouge due to the large number of cores on CPU's and the use of
massively parallel dedicated chips like video cards.

It's even possible to crack passwords, and do machine learning, and
mine crypto currencies on dedicated chips.

*** Dynamic Programming

Applied to recursion is descent + memoization recursively can be no
cycles in the DAG of the recursion, or it will get into an infinite
loop. It ss fundamentally a brute force approach good for computing
min/max style answers.

*** Greedy Programming

Packing algorithms, like the parser compiler function packer.
My Emacs Parser Compiler used a greedy technique with
push back to maximally fill functions with code [[cite:&MattieParser]].


*** Lazy Programming
When the computation may not be needed
When the problem cannot fit into memory it can be lazy loaded as needed


*** Streams 

Streams [[cite:&SICPstreams]] are a finite sequence of discrete elements
of the same type processed in a linear sequence of operations. Good
for representing large data sets coming out of storage.

* Sketch the Code

Sketch the code in functions, loops, with comments on purpose and
O-notation complexity

- *Initialize*: establish a return value, empty containers over nulls
- *Terminate*: determine the base case. When is it done?
- *First, Common, Last Cases*: The basic sequence of the algorithm
- *Corner*: cases 
- *Input Validation*: System errors, stale state, deadlocks, and sync errors, timeouts
- *Invariants*: statements always true in the procedure’s execution
- *State*: initialize, update, delete [[cite:&SICPcostOfAssignment]]

Parts of the problem must not be interdependent.

* Data Structures

** Array

Typed and RAM indexed they are extremely fast with O(1) read for any
element Insert is very slow as the array elements have to be copied to
make room for the element allows the use of fast algorithms like
binary search

** List

single or double linked for traverse forward and traverse back, fast
inserts can only efficiently access in a linear way random access is
$\theta(n)$ counting length is 0(n) double linking requires twice as
much overhead

** Trees

good for storing hierarchal data natural fit for recursive algorithms
good for indexes requires only O logx(n) to find an element.

Performance is maintained only when the tree is balanced, re-balancing
on insert can be an expensive operation recursion is practical to the
logarithmic complexity of traversal

** Stack

Stacks are an excellent structure for back-tracking problems. they
are LIFO.

** LIFO (Last in First out) 

push on the end, pop by removing from end. Fast implementation in
arrays.

** QUEUE FIFO (First in First out)

Good for processing in chronological ordering Can be used to do a
breadth traversal of a tree

** Hashes

A bread and butter data structure used pervasively to look up
non-integer keys in $\theta(1)$ complexity.

#+print_bibliography:
