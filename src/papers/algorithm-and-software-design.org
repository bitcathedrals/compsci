#+LATEX_CLASS: article

#+TITLE: Algorithms and Software Design
#+AUTHOR: Mr. Mattie

* Software: Design and Implementation

#+BEGIN_CENTER
*Correct Thinking leads to correct Code!*
#+END_CENTER

Algorithms and Software problems, even the ones that seem simple, can
be very difficult to design and implement.

Purely technical algorithms frequently have mistakes in them until
they have been debugged over a significant period of time. Business
Logic in software is unique to the domain and there is little guidance
there to help the developer.

The biggest failing is not one of coding, but rather a failure of
imagination, a failure to conceive of all the scenarios and aspects
of the use of the algorithms.

Not realizing that a list might be empty, missing a subtle aspect of
definition that impacts the solution, or miscalculating the complexity
of a naive solution - these oversights can wreck an algorithm or
system.

We will go through a systematic process that is a discovery processs,
finding all the ways the algorithm will be used and stretched. The
code is intuitive, or at least tractrable, once all the facets of the
problem domain have been discovered.

* Requirements - Building the Domain Model
#+BEGIN_CENTER
/State & Clarify/ – Deep Comprehension Modeling
#+END_CENTER

Domain Modeling is understanding the problem, in the language of the
problem.

** Comprehension

Study the given requirements with deep comprehension, looking to find
a singular descriptive model, describing in entities and relationships
the richness of the problem. Look for all the quirks, ambiguities, and
anomolies of the problem.

The clarity of your understanding should reveal and resolve into a
cohesive set of entities and relationships in the domain language.

Be very wary of premature optimization. This initial model is for
understanding the problem fully, and proving correctness of
the computational model that follows.

** Solution

With the domain model, translate the entities into types, and
relationships as operations on a type. Ideally the syntax
of overloading operators facilitates an intuitive, logical, and well
understood way of using the types.

Designing types, with value semantics expressed in interrelated
operations, organized along paradigm lines, build intuitive use and
composition.

Types and operations that are intrinsicly interrelated, or tied to
shared resources should be put into modules. Modules do not have to be
a large collection, in fact a large number of small modules makes it
easier to test code and manage complexity to a large degree.

** Structuring

The structuring of the program has a large impact on how easy to
develop, test, and maintain. Modules are the basic level of
construction. Inside modules interrelated types internally construct
and consume each other's types directly.

Modules present their functionality in one of three ways:

*** Types and Operations

Exported types, operations, and their interactions with each other,
are directly consumable. This technique is Layering.

With a schema for types, a paradigm for operations, and syntatic
constructs for composition, layering is a powerful API.

This type and paradigm interface has a high level of coupling, and is
difficult to test, maintain, and change due to the diffusion of direct
consumption throughout the consumer. For these reasons it should not
be chosen lightly.

*** Dependency Inversion

**** Citations

- [[cite:DependencyInversion][DI General]] 
- [[cite:DependencyInjectionPython][Python DI]] 
- [[cite:DependencyInjectionC++][DI C+]]

**** Links

- [[https://python-dependency-injector.ets-labs.org/introduction/di_in_python.html][DI in Python]]
- [[https://github.com/google/fruit/wiki][Fruit: Google DI for C++]]

For intra-module coupling that does not rely on the consumer creating
API types, Where a Singleton in the module API, or a Singleton that
constructs all related types is usable, Dependency Inversion is the
preferred approach.

Dependency Inversion is where the code that uses the object does not
create types it consumes; instead consumed types are constructed in
the orginating module, and the resulting the object is passed to the
consumer at construction. This allows the entirety of the API to be
hidden behind a abstract interface.

For example, a module for configuration could contain the information
needed to construct resource objects such as Database objects, or OS
objects.

Those resource objects would use the configuration objects for
instantiation, and as a singleton into the resource can be easily
injected wherever it is needed.

Instead of orchestrating the construction of a resource type, the
consumer would use it through an abstract interface, and would receive
the consumed object at construction.

In this paradigm all three modules, the configuration, the resource,
and the consumer module are loosely coupled through abstract
interfaces.

When this technique is used there is no need for extensive and
complicated mocking for testing, use is well defined, configuration
and construction is implemeneted in one place; instead of fanning out
throughout the consumer.

The code is easily written, maintained, changed, and tested.

*** Message/Event Passing

System level design should define intermodule communication as message
passing protocols. With dedicated API types decoupling the interface
from the logic and solving code, the API and logic can evolve
idependently.

messages/events should be simlple declarative types with some kind of
versioning in how they are named, so type based dispatch can be used
in an API object to process the message into the system.

** Testing

Testing solutions is vital as any code of significant complexity is
broken as first conceived.

Testing can be challenging and labor intensive when the type and
paradigm API is used. You would have to use a number of tests with
fixtures (pre constructed sets of types) to test it. Unit Testing
isolation techniques and infrastructure is necessary.

Unit Testing is effective, and has been beneficial, but the mass of
test code becomes baggage that discourages change, due to the amount
of work to update it, especially in a layering API.

With a system that is designed as a large number of small modules, the
kind of isolation techniques typical in Unit Testing are not needed,
except for tracing a specific issue. It can be mocked through
dependency inversion.

Instead all testing as much as possible should be black-box, without
any insight to the component (type/module) under test. With a large
number of test sets black box can be effective at assuring that the
code is correct. Coverage analysis should be used to ensure that the
testing is in fact covering most of the code.

Constructing tables of test data keeps the testing code flexible and
thorough. Fuzzing is helpful for spotting corner cases that have been
missed by pure analytical test cases.

* SCENARIOS 

#+BEGIN_CENTER
/State & Clarify/ – CASES and EXPECTATIONS
#+END_CENTER

The Application Layer of the algorithm or system defines how it is
used, and what it is expected to do. This is interaction and
expectation at a high level of granularity.

** USE CASES - (Contexts & Inputs)

CASES are Contexts and Inputs. Contexts are factors or constraints
that shape the case beyond the input that is fed into the algorithm or
system. Inputs are events and data that the solver consumes to produce
a result in the EXPECTATIONS.

*** INPUTS?
#+BEGIN_CENTER
/State & Clarify/ - Types and Scale
#+END_CENTER

The type, scale, and possible anomolies in the inputs to the algorithm
or system have a huge impact on the design. Designing something for
one thousand elements is a very different from designing for one
million elements. A thousand will fit easily to memory, a million
elements is a different design entirely.

*** RETURN?
#+BEGIN_CENTER
/State & Clarify/ - Results - (Entities and Constraints)
#+END_CENTER

Entities of the EXPECTATIONS are the other side of the coin, and a
crucial aspect of design. BEHAVIOR cannot be solved correctly without
knowing the beginning and the end.

*** CHANGE?
#+BEGIN_CENTER
/State & Clarify/ - Before and After [[cite:SICPcostOfAssignment][SICP Cost of assignment]]
#+END_CENTER

Sometimes the algorithm must make a scoped and persistent change in
the system itself. This is less-desirable from a design and
implementation standpoint, but if it is the EXPECTATION then it must be
done well.

A good way of providing some formalism to describing state changes is
Predicate Transformers [[cite:PredicateTransformers][Predicate Transformers]] 
by _Edsgar Dijkstra_ that have a pre-assertion, the change, and then 
a post-assertion of what the state looks like before, changed, and after. 
This level of formalism is not usually necessary unless you are dealing with 
complex state change issues like parallelism.

If a change must be made it is best to make the algorithm idempotent,
or where repeated calls have the same result. For example: a light
button as a toggle will alternate on/off the lights every time it's
pressed. This is confusing if you simply want it to turn on, or turn off.

A proper switch instead, will turn the light off every time it is
pressed in the off direction, and on when pressed on, no matter how
many times it's pressed. That is idempotent.

** EXPECTATIONS
#+BEGIN_CENTER
/State & Clarify/ - What is the desired outcome?
#+END_CENTER

EXPECTATIONS and their qualifications are the definition of what
correctly solves the CASE. They are what the algorithm should compute,
do, or return, with type and scale of results. The qualifications are
constraints on the solution such as latency, memory consumption, or
resource utilization.

** BEHAVIOR
#+BEGIN_CENTER
/State & Clarify/
#+END_CENTER

BEHAVIOR is the business logic and core logic, that from lead from
inputs, events and data, to producing the EXPECTATIONS. It is vital to
clarify the behavior and make sure it covers all the richness and
facets of not only the inputs, but the outputs and changes.

* Sketch the Code

Sketch the code, or module in functions and loops, with comments on
purpose and O-notation complexity

1. *Initialize*: establish a return value, empty containers over nulls
2. *Terminate*: determine the base case. When is it done?
3. *First, Common, Last Cases*: The basic sequence of the algorithm
4. *Corner*: cases 
5. *Input Validation*: events, values, completeness, and ranges
6. *State*: initialize, update, delete, and lifetime.

* Design (Iteration)

** Refine Types
#+BEGIN_CENTER
Minimze Semantics of Types, and define operations in paradigm concepts
#+END_CENTER

Define types as constant or mutable that have essential cohesion,
where their definition of cohesion is perfectly minimal, in that they
can only be defined with their set of interrelated properties, but
have no properties that are not intrinsic to the type value semantics.

** Isolate Operations
#+BEGIN_CENTER
Maximize Idempotent side-effect free operations [[cite:SICPcostOfAssignment][SICP Cost of assignment]]
#+END_CENTER

Breakdown interaction of types into paradigm derived operations, and
try and maximize side-effect free functions. Where there is state
handle it carefully defining the entire life-cycle of the state in
entities.

** Build Modules
#+BEGIN_CENTER
Sets of interrelated types and operations that share resources are Modules
#+END_CENTER


Modules are are interrelated types that construct each other, and
share resources. If a type can stand independently it belongs
elsewhere.

** Message Passing (API)
#+BEGIN_CENTER
APIs are message passing between functionally isolated components (API)
#+END_CENTER

API's pass declarative and constant messages/events between modules
that are described in protocols and modeled as seqeuence diagrams.

** Paradigm

#+BEGIN_CENTER
/Solution Comprehension/
#+END_CENTER

Paradigm is what model best describes the problem (dynamic greedy,
lazy, streams, Relational, divide and conquer) and most efficiently
produces an answer.

Spot check the paradigm against the CASES to see if it adequately
describes the problem. Find the right paradigm.

*** Recursion

\begin{equation}
\theta(\log_n)
\end{equation}

Recursion is elegant and compact. In languages that support it, it
simplifies and strips the implementation down to the core logic.

**** recurrence

Distill the problem down into a solution that can be applied to all
the elements.

**** termination

Define the base case or *termination* as return of the solution that
unwinds the recursion.

*** Divide & Conquer [[cite:IntroDivideAndConquer][Intro to Divide and Conquer]]

\begin{equation}
\theta (n * \log_n)
\end{equation} 

Divide and Conquer is a technique where the problem is dived into
parts, each part is solved, and then the sub-solutions are combined
into the complete solution.

Divide and conquer is also a natural fit with parallel implementations.

**** Decide the granularity of the division, divide the problem into $n/x$ parts.

**** Solve the sub-problem. The reduced scale of /n/ reduces the complexity or run time of the solution. [[cite:&IntroRecurrences]]

**** Combine the solutions for the final solution

*** Dynamic

Dynamic Programming uses a technique of caching answers to frequently
computed problems.

Memoization [[cite:IntroMemoization][Memoization]] is a powerful technique and
in Python the "functools" package has a LRU [[cite:PythonLRU][Python LRU]]

*** Linguistic (DSL)

DSL stands for Domain Specific Languages. Thes can be simple
declarative language processors, or full blown domain specific
languages like "R" [[cite:WikiR][R language]]. They can be used to define complex
problems, and through the language implement a powerful and flexible
solver.

*** Query

Query Languages like SQL can go beyond transactional into the space of
analytical queries either providing processing of data, or even
computations such as "GROUP BY" and MIN and MAX in SQL [[cite:WikiSQL][SQL Introduction]].

The underlying model behind relational databases is the Relational
Algebra [[cite:codd2021relational][Codd Relational Paper]]

*** Logic

Logic systems are basically rule systems like Prolog [[cite:WikiProlog][Prolog Introduction]]
They are used in mathematical and logic applications. Their solution
finding approach can also be useful in solving difficult problems like
cross-wiring network links for redundancy and expert systems.

*** Single Pass

Single pass approaches are significant when the data set is so large
it cannot be contained in memory. These kinds of problems are becoming
more important as the size of data in general skyrockets.

*** Multi-Pass

Sometimes huge gains can be made by making multiple passes. This is
basically a variant on Dynamic Programming. Database Indexes are a good
example, when the data queried be found quickly in the index instead
of a full table scan.

Sorting ahead of time makes possible binary searches or tree
alogorithms for searching. If the search is executed many more times
than the routine to maintain the order of the index, a massive
performance increase can be realized.

*** Pre-Compute

Pre-Computing unlike multi-pass where the complete problem
set is traversed, is instead the compilation of tables that
are expensive to compute. 

In the early days of computing the computation of sine/cosine and
other graphic operations were prohibitely expensive.

Since the answers were a small table pre-computing the equations
greatly sped up programs. Bitmaps were even compiled to machine code
for faster rendering.

*** Dynamic Programming

Applied to recursion is (descent + memoization) recursively can be no
cycles in the DAG of the recursion, or it will get into an infinite
loop. It is fundamentally a brute force approach, good for computing
min/max style answers.

*** Greedy Programming

Greedy algorithms, like the parser compiler packer function I wrote
in my Emacs Parser Compiler used a greedy technique with
packing to maximally fill functions with code [[cite:MattieParser][Mattie ELisp Parser Compiler]].

*** Lazy Programming

When the computation may not be needed or when the problem cannot fit
into memory it can be lazy loaded, or lazy computed. Here the sequence
is produced on demand through a generator function with a internal
state that is updated when a value is produced, streaming values from
a compact single value generator.

*** Parallel Programming

Parallel programming [[cite:ParallelAlgorithms][Parallel Algorithms]] is a technique
implemented in hardware with things like hyperthreading and multiple
cores. Even basic functions like add instructions can be implemented
in parallel.

Fundamentally parallel algorithms [[cite:ParallelGraphModel][Parallel Graph Model]]
exploit the ability of systems and software to execute two or more
pieces of code simultaneously.

If the problem can be partitioned into seperate tasks, or ultimately
partitioned and solved along the lines of the divide and conquer
class of algorithms, massive speed ups are possible.

Fundamentally parts that are readers will always "block" or stop
parallel execution because they cannot proceed without the
values to compute their next step. Writers do not need to block
necessarily, but to maintain integrity they would.

When a program needs to synchronize to a single execution the
work being done this serialized section is called a "critical section".

I won't go into these vast details except to say that protecting value
intergrity between threads and implementing critical sections is very
complicated and error prone. The common model is to implement
integrity in a database and use concurrent processes that don't share
memory.

*** Streams 

Streams [[cite:SICPstreams][SICP Stream Model]] are a finite sequence of discrete elements
of the same type processed in a linear sequence of operations. What
makes streams unique is that all of the types are consumers of the
same stream type and are producers as well allowing them to be
chained.

* Data Structures

** Array

Typed and indexed they are extremely fast with O(1) read/write for any
element. Insert is very slow as the array elements have to be copied
to make room for each insertion. The equal cost of access to any
element makes algorithms like binary search, and some sorting
algorithms possible.

** List

Single or Double Linked lists have efficient inserts but perform
poorly in most cases.

Counting length or adding to end is $\theta(n)$

** Trees

Good for storing hierarchal data and a natural fit for recursive
algorithms, trees require only $\theta \log_n$ to find an element.

Performance is maintained only when the tree is balanced, re-balancing
on insert can be an expensive operation.

** Stack/LIFO 
#+BEGIN_CENTER
Last In First Out
#+END_CENTER

Stacks are an excellent structure for back-tracking problems. They
are LIFO, or Last In First Out. They can be used as a substitute
for recursion, and generally for back-tracking.

** QUEUE FIFO
#+BEGIN_CENTER
First In First Out
#+END_CENTER

Good for processing in chronological order. It can also be used for
a breadth traversal of a tree.

** Hashes

A bread and butter data structure used pervasively to look up
non-integer keys in $\theta(1)$ complexity.

A bread and butter data structure used pervasively to look up
non-integer keys in $\theta(1)$ complexity.

* Competitive Algorithm Coding

Here is a short condescend set of principles for competitive
algorithm coding.

** Redefine the problem in comments.

Carefully restate in comments the problem description in the
mechanical terms of the problem statement, and the definition
of a solution.

** Find the simple solution

Taking care to understand the O Complexity of the problem as basically
stated in comments find the difficult part of the challenge, which is
usually some kind of combinitorial complexity.

** Devise a fast solution

Once the fast solution is devised you can proceed to implementation.

** Sketch the Code

Sketch the code, or module in functions and loops, with comments on
purpose and O-notation complexity

1. *Initialize*: establish a return value, empty containers over nulls
2. *Terminate*: determine the base case. When is it done?
3. *First, Common, Last Cases*: The basic sequence of the algorithm
4. *Corner*: cases 
5. *Input Validation*: events, values, completeness, and ranges
6. *State*: initialize, update, delete, and lifetime.

** Test/Debug

Test and debug with print statements, never delete a print statement,
just comment them out.

#+print_bibliography:
